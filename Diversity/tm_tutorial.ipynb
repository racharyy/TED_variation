{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rupamacharyya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rupamacharyya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['physically', 'base', 'approach', 'shape', 'blending']\n",
      "['incremental', 'maintenance', 'distributive', 'aggregate', 'function']\n",
      "['asynchronous', 'spike', 'event', 'coding', 'scheme', 'programmable', 'analog', 'array']\n",
      "['voltage', 'syllabic', 'companding', 'domain', 'filter']\n",
      "['reduce', 'complexity', 'space', 'frequency', 'model', 'multi', 'channel', 'application']\n",
      "['cognitive', 'mobile', 'virtual', 'network', 'operator', 'investment', 'pricing', 'supply', 'uncertainty']\n",
      "['predict', 'click', 'estimate', 'click']\n",
      "['supporting', 'list', 'model', 'timely', 'approach']\n",
      "['programming']\n",
      "['resilient', 'right', 'protection', 'sensor', 'stream']\n",
      "['novel', 'hybrid', 'neuro', 'wavelet', 'system', 'robust', 'speech', 'recognition']\n",
      "['image', 'sensor', 'using', 'variable', 'reference', 'domain', 'encoding']\n",
      "['power', 'minimization', '433-mhz', 'implantable', 'neural', 'recording', 'system']\n",
      "['distribute', 'exponentially', 'weight', 'split']\n",
      "['towards', 'legged']\n",
      "['maximum', 'coverage', 'minimum', 'multi', 'domain', 'network']\n",
      "['approximate', 'query', 'stream', 'guarantee', 'error', 'performance', 'bounds']\n",
      "['27-ghz', 'power', 'tuning', 'range']\n",
      "['effect', 'skew', 'access', 'buffer', 'contention', 'sharing', 'environment']\n",
      "['memory', 'efficient', 'scalable', 'video', 'encoder', 'architecture', 'multi', 'source', 'digital', 'environment']\n",
      "['sybil', 'attack', 'mobile', 'user', 'friend', 'rescue']\n",
      "['scalable', 'lookup', 'programmable', 'router']\n",
      "['graphics', 'internet', 'shoot', 'panel', 'session']\n",
      "['automate', 'interior', 'design']\n",
      "['efficiency', 'collaborative', 'cache', 'aware', 'network']\n",
      "['diffusion', 'dynamic', 'network']\n",
      "['flexible', 'dialogue', 'system', 'enhance', 'usability']\n",
      "['click', 'create', 'animation']\n",
      "['power', 'decoding', 'base', 'iteration', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "text_data = []\n",
    "with open('dataset.csv') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(18, 1), (19, 1), (20, 1), (21, 1), (22, 1)], [(23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)], [(31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1)], [(40, 2), (41, 1), (42, 1)], [(0, 1), (27, 1), (43, 1), (44, 1), (45, 1)], [(46, 1)], [(47, 1), (48, 1), (49, 1), (50, 1), (51, 1)], [(52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1)], [(19, 1), (50, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1)], [(58, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1)], [(71, 1), (72, 1), (73, 1), (74, 1)], [(75, 1), (76, 1)], [(19, 1), (28, 1), (34, 1), (77, 1), (78, 1), (79, 1)], [(51, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1)], [(69, 1), (86, 1), (87, 1), (88, 1)], [(89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1)], [(28, 1), (93, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1)], [(33, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1)], [(15, 1), (101, 1), (109, 1), (110, 1)], [(111, 1), (112, 1), (113, 1), (114, 1), (115, 1)], [(116, 1), (117, 1), (118, 1)], [(34, 1), (119, 1), (120, 1), (121, 1), (122, 1)], [(34, 1), (123, 1), (124, 1)], [(58, 1), (125, 1), (126, 1), (127, 1), (128, 1)], [(40, 1), (129, 1), (130, 1)], [(1, 1), (69, 1), (131, 1), (132, 1), (133, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
